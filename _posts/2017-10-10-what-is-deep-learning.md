---
layout: post
title: "딥러닝이란 무엇인가?"
date: 2017-10-10 09:00:00 +0900
author: kilho_kim
categories: [machine-learning]
tags: [machine-learning, data-science]
comments: true
name: what-is-deep-learning
---

수아랩 리서치 블로그 두 번째 글의 주제는 '딥러닝이란 무엇인가?' 입니다. 오늘날의 AI(인공지능)를 가능하게 하는 기술이 '딥러닝'이라고들 하는데, 왜 수아랩 리서치 블로그에서 '머신러닝'부터 언급하고 '딥러닝'은 뒷전에 뒀는지 의아한 분들이 많으실 것으로 생각합니다. 이 글을 통해 그 궁금증을 풀어드리겠습니다.

## 서론
요즘에 **딥러닝(Deep Learning)**이란 단어가 여기저기에서 많이 들려옵니다. 머신러닝(machine learning)과 왠지 느낌은 비슷한데, '딥'하다는 수식어가 붙어서 뭔가 좀 더 심오해(?) 보이기도 합니다. 

오늘날 딥러닝이라는 단어는 대부분 AI의 꼬리표처럼 등장하는 경우가 많습니다. 본래 딥러닝은 2016년 초까지는 아는 사람만 아는 단어였는데, 국내에서는 특히 2016년 3월 바둑 두는 기계인 '알파고(AlphaGo)'가 대한민국의 이세돌 9단을 바둑으로 압승하면서 널리 알려졌습니다. 세간에서는 AI의 괄목할 만한 발전을 보면서 딥러닝의 무궁무진한 가능성을 높게 보는 사람들이 늘어났고, 좀 더 상상력이 풍부하신 분들은 이를 보며 '인간이 기계 제국에 지배당할 날이 머지 않았다'는 우려까지 하시게 된 것 같습니다.

> "딥러닝 썼더니 바둑도 잘 두던데? 우리 비즈니스에도 딥러닝 적용하면 전부 대체 가능하겠네!!"

이러한 변화의 길목에서, 필자는 개인적으로 '딥러닝 만능주의'가 생겨나고 있다는 느낌을 지울 수 없습니다. 실제로 적지 않은 국가 혹은 기업의 의사결정권자들이, 딥러닝의 성공적인 적용 사례만을 보고 (위와 같은 뉘앙스로 말씀하시면서) 호기롭게 딥러닝을 자신들의 비즈니스에도 적용해보자는 주장을 하시는 것을 심심치 않게 보고 들어 왔습니다. 그러나, 현재의 딥러닝에는 엄연한 한계가 존재하며, 아직까지는 특정 부류 업무의 자동화를 위한 하나의 도구로 보아야 합당합니다. 

이번 글에서는 이러한 오해를 불식하고자, 딥러닝이란 기술이 본질적으로 무엇인지, 어떠한 문제에 효과적으로 적용될 수 있는지 등을 중심으로 이야기해 보고자 합니다.


## 딥러닝 ⊂ 머신러닝

결론부터 얘기하자면, *딥러닝은 머신러닝의 세부 방법론들을 통칭하는 개념에 불과*합니다. 즉, 이론적으로 딥러닝은 머신러닝의 '부분집합'이라고 할 수 있으며, 사실 기존 머신러닝 이론에서 크게 새로울 것이 없습니다.

보다 자세한 설명을 위해서는 좀 더 수학적인 설명이 필요한데, 이에 앞서 <a href="http://research.sualab.com/machine-learning/2017/09/04/what-is-machine-learning.html" target="_blank">바로 이전 글</a>에서 살펴본 **머신러닝의 핵심 요소**를 다시 한 번 나열해보도록 하겠습니다.

### 머신러닝의 핵심 요소

- 데이터
- 러닝 모델 (+러닝 알고리즘)
- 요인 추출

엄밀히 말하면 요인 추출(feature extraction)은 *필수적인* 것은 아니나, 머신러닝의 성패를 크게 좌우하는 것이라 *핵심 요소* 리스트 상에 추가해 보았습니다. 머신러닝을 위해서는 데이터와 러닝 모델이 필요하며, 성공적인 학습을 위해서는 우선 데이터를 면밀히 조사하고 그 특징을 파악한 뒤, 이에 가장 적합한 러닝 모델을 잘 골라야 하며, 원 데이터를 그대로 사용하는 것보다는 효과적인 요인을 새로 정의하여, 이를 추출한 뒤 러닝 모델에 입력해주는 것이 좋다고 하였죠. 

### 퍼셉트론 - 선형 모델의 일반화

이전에 러닝 모델의 예시로 든, 가장 단순한 러닝 모델인 **선형 모델(linear model)**을 기억하시나요? 2개의 입력 변숫값으로 구성된 2차원 데이터 예시 $$(x_1, x_2)$$를 받아들여 하나의 출력 변숫값 $$f(x_1, x_2)$$을 출력하는 다음과 같은 함수를 예로 들었습니다:

\begin{equation}
f(x_1, x_2) = w_0 + w_1x_1 + w_2x_2
\end{equation}

저번 글에서의 '3개월 내 채무 이행 여부 예측 기계' 예시에서는, 어느 특정한 예시 $$(x_1, x_2$$에 대하여, 학습된 러닝 모델의 출력값 $$f(x_1, x_2)$$가 0보다 크면 'O'로, 작으면 'X'로 분류했던 바 있습니다.

{% include image.html name="what-is-machine-learning" file="age-to-salary-classify-sample-plot.png" description="3개월 내 채무 이행 여부 예측 결과 예시" class="medium-image" %}

선형 모델을 매번 위와 같은 수식으로 표현하면 한 번에 알아보기 불편하므로, 아래와 같이 좀 더 예쁜 그림으로 대신 표현하도록 하겠습니다.

{% include image.html name=page.name file="linear-model-diagram.svg" description="선형 모델의 도식화 예시" class="large-image" %}

위 그림을 보시면, 기본 뼈대에 $$w_0$$, $$w_1$$, $$w_2$$가 있고, 여기에 $$1$$, $$x_1$$, $$x_2$$가 각각 곱해진 후, 이들을 모두 합산한($$\sum$$) 결과를 그대로 출력하고 있습니다. 여기까지가 위에서 사용한 선형 모델인데, 일반적으로는 합산 결과에 **활성함수(activation function)**라는 모종의 함수 $$\sigma(\cdot)$$를 적용한 결과를 최종 출력값으로 대신 출력하는 경우가 많습니다. 여기에, d개의 입력 변숫값으로 구성된 *$$d$$차원* 데이터 예시 $$(x_1, x_2, ..., x_d)$$를 받아들이는 상황을 가정하면 아래와 같은 일반화된 그림을 표현할 수 있습니다.

{% include image.html name=page.name file="perceptron-diagram.svg" description="퍼셉트론" class="full-image" %}

일관성을 위해, 기존에 $$w_0$$이 등장하는 항에 $$x_0=1$$이 곱해진다고 가정하면, 입력 벡터 $$\boldsymbol{x}=(x_1, x_2, ..., x_d)$$를 받아들인 뒤 각 성분에 가중치를 곱하고, 그 결과를 모두 합산한 후, 활성함수 $$\sigma(\cdot)$$을 적용한다고 할 수 있습니다. 전체 과정을 수행하는 이 선형 모델을 하나의 함수 $$h(\cdot)$$로 나타낼 수 있으며, 이를 **퍼셉트론(perceptron)**이라고 부릅니다. 이 때, $$x_0=1$$까지 포함된 $$(d+1)$$차원 벡터를 받아들이므로 이를 편의 상 '$$(d+1)$$차원 퍼셉트론'으로 부르겠습니다. 당연히, 활성함수로 항등함수 $$\sigma(s)=s$$를 사용할 경우 이것은 위에서 살펴본 선형 모델이 될 것입니다. 

> 선형 모델은, 사실 퍼셉트론의 특수한 형태로 해석할 수 있습니다.

퍼셉트론의 모양을 가만히 살펴보면, 이는 신경계의 기본 단위인 <a href="https://ko.wikipedia.org/wiki/%EC%8B%A0%EA%B2%BD_%EC%84%B8%ED%8F%AC" target="_blank">**뉴런(neuron)**</a>과 그 모양이 매우 흡사합니다. 뉴런은 주변으로부터 자극을 받아들이는 돌기가 여러 개 뻗어 있으며, 이러한 자극들이 중심부의 핵으로 모여든 후 처리되고, 그 결과물을 다른 하나의 돌기를 통해 다른 곳으로 전달하도록 구성되어 있습니다. 이러한 유사성 때문에, 퍼셉트론을 흔히 인공 뉴런(artificial neuron) 혹은 줄여서 뉴런이라고도 부릅니다.

{% include image.html name=page.name file="neuron.png" description="뉴런의 구조" class="large-image" %}

### 인공신경망 - 퍼셉트론의 조합 및 확장의 결과

신경계에서의 뉴런들은 그 수가 엄청나게 많으며, 서로간에 매우 복잡한 구조로 얽히고설켜 하나의 거대한 망을 구성하는데, 이를 신경망(neural network)이라고 합니다. 머신러닝 과학자들은 신경계의 이러한 신경망 구조에 착안하여, 퍼셉트론을 하나의 빌딩 블록(building block)이라고 생각하고, 여러 개의 퍼셉트론을 아래 예시와 같이 연결한 **인공신경망(artificial neural network)**을 고안하였습니다.

{% include image.html name=page.name file="neural-network-detailed.svg" description="4개의 퍼셉트론 간의 2차원적 연결 예시" class="full-image" %}

위 그림에서 입력 벡터 $$\boldsymbol{x}=(x_1, x_2, ..., x_d)$$가 각각 상단, 중단, 하단에 위치한, 서로 다른 $$(d+1)$$차원 퍼셉트론의 입력값으로 들어갑니다(이를 각각 초록색, 붉은색, 보라색으로 표현했습니다). 각각의 퍼셉트론에서 가중합 및 합성함수를 거친 3개의 출력값이 $$x_0=1$$과 함께 4차원 입력 벡터를 구성하며, 4차원 퍼셉트론으로 입력됩니다. 4차원 퍼셉트론에서도 최종적으로 가중합 및 합성함수를 거치고, 최종적으로 1개의 값을 출력합니다. 

위와 같이 연결한 구조에서, 두 개의 **층(layer)**을 확인하실 수 있나요? 3개의 $$(d+1)$$차원 퍼셉트론이 나란히 붙어 있는 부분을 첫 번째 층, 1개의 4차원 퍼셉트론이 있는 부분을 두 번째 층으로 보시면 됩니다. *낮은(앞쪽) 층*의 퍼셉트론에서 출력된 값들이, *높은(뒷쪽) 층*의 입력값으로 들어가는 구조를 확인할 수 있습니다. 

더 거대한 인공신경망 구조를 표현하기 위해, 지금부터는 하나의 퍼셉트론을 아래와 같이 좀 더 단순화시켜 표현해 보겠습니다. 그리고, 그림 상에서 입력값 또는 출력값을 나타내는 원형 부분을 **노드(node)**라고 표현할 것입니다.

{% include image.html name=page.name file="perceptron-simplified-diagram.svg" description="단순화된 퍼셉트론 구조" class="large-image" %}

그러면 총 2개의 층으로 구성되어 있으며, 첫 번째 층에는 $$(d+1)$$차원 퍼셉트론이 $$H$$개, 두 번째 층에는 $$(H+1)$$차원 퍼셉트론이 $$K$$개 있는 인공신경망을 아래와 같이 표현할 수 있습니다. *이 때, 가장 낮은 층의 노드 및 $$+1$$로 표시된 부분은 퍼셉트론이 위치하지 않으므로 주의하시길 바랍니다!*

{% include image.html name=page.name file="multilayer-perceptron.svg" description="다층 퍼셉트론의 일반적 구조" class="medium-image" %}

입력 벡터가 자리잡는 층을 **입력층(input layer)**, 최종 출력값이 자리잡는 층을 **출력층(output layer)**, 입력층과 출력층 사이에 위치하는 모든 층을 **은닉층(hidden layer)**이라고 합니다. 위 그림 상에서는 3개의 층이 존재하지만, 층의 개수를 셀 때 입력층은 생략하는 것이 기본입니다. 따라서 위 구조에서는 총 *2개*의 층이 존재한다고 할 수 있습니다. 퍼셉트론을 기본 빌딩 블록으로 하여, 이런 패턴에 따라 2차원적으로 연결되어 구성되는 인공신경망의 일종을 특별히 **다층 퍼셉트론(multi-layer perceptron)**이라고 합니다.

이런 입력층-은닉층-출력층의 경우, 다층 퍼셉트론뿐만 아니라, 좀 있다 설명할 다양한 인공신경망 구조에서 공통적으로 존재하는 층입니다. 은닉층의 개수가 많아질수록 인공신경망이 *'깊어졌다(deep)'*고 부르며, 이렇게 *충분히 깊어진 인공신경망을 러닝 모델로 사용하는 머신러닝 패러다임*을 바로 **딥러닝(Deep Learning)**이라고 합니다. 그리고, 딥러닝을 위해 사용하는 충분히 깊은 인공신경망을 **심층 신경망(Deep neural network)**이라고 통칭합니다.

'그럼 은닉층 및 출력층이 몇 개 이상이 있어야 심층 신경망이냐?'는 의문이 생길 수 있는데, 일반적으로는 *은닉층+출력층이 2개 이상*이 되면 심층 신경망이라고 합니다. 예를 들어, 아래와 같이 8개 은닉층+출력층으로 구성된 다층 퍼셉트론은 심층 신경망입니다. 

{% include image.html name=page.name file="deep-neural-network-example.svg" description="심층 신경망 예시" class="full-image" %}

이제 여러분들은, *딥러닝은 머신러닝의 세부 방법론들에 불과하다*는 말의 의미를 이해하셨을 것이라고 생각합니다. 머신러닝의 큰 틀은 그대로 가져가되, 러닝 모델로 '충분히 깊은' 인공신경망을 사용하고, 이에 맞는 러닝 알고리즘을 사용하여 러닝 모델을 학습한 경우 '딥러닝을 했다'고 표현해도 크게 무리가 없습니다.


## 대표적인 딥러닝 모델

심층 신경망의 기본 단위는 퍼셉트론이라고 하였습니다. 바로 앞에서는 다층 퍼셉트론 구조를 소개하였는데, 복수 개의 퍼셉트론을 서로 어떻게 연결하느냐에 따라 그와는 다른 새로운 구조를 형성할수도 있습니다. 오늘날, 다루고자 하는 데이터의 속성에 따라 효과적으로 적용할 수 있는 특수한 구조의 심층 신경망이 여럿 발표되었는데, 그 중 현재 가장 많이 쓰는 것 3가지를 소개해 드리고자 합니다.

### 완전 연결 신경망

**완전 연결 신경망(fully-connected neural network)**은, 사실 앞서 소개했던 *다층 퍼셉트론을 지칭하는 또 다른 용어*입니다. 다만 여러 구조의 심층 신경망이 추가로 발표되면서 기존의 다층 퍼셉트론이라는 표현을 사용하기 다소 애매해졌고, 이에 따라 오늘날에는 완전 연결 신경망이라는 표현을 널리 사용하고 있습니다.

완전 연결 신경망은, 위의 *다층 퍼셉트론의 일반적 구조*에서와 같이 노드 간에 횡적/종적으로 2차원적 연결을 이룹니다. 이 때, 서로 같은 층에 위치한 노드 간에는 연결 관계가 존재하지 않으며, 바로 인접한 층에 위치한 노드들 간에만 연결 관계가 존재한다는 것이 핵심적인 특징입니다.

### 컨볼루션 신경망

완전 연결 신경망에서 하나의 층 내부만을 보면, 그 안에 위치한 노드들은 1차원적으로 세로 방향으로만 배치되어 있습니다. 만약 완전 연결 신경망의 하나의 층에 위치한 노드들이 2차원적으로 가로/세로 방향으로 동시에 배치되어 있다면 어떤 모습일까요? 각 층에 가로 $$w$$개, 세로 $$h$$개의 노드가 배치되어 있는 2층짜리 완전 연결 신경망을 생각해 봅시다.

{% include image.html name=page.name file="2d-deep-neural-network.svg" description="2차원적 완전 연결 신경망*<br><small>(*주의: 지면 관계상, 노드 간의 연결 관계에 대한 표현 및 $$+1$$ 노드는 생략하였습니다.)</small>" class="full-image" %}

위 그림과 같이 노드를 배치할 경우, 노드 간의 연결 개수가 막대하게 증가한다는 문제가 있습니다. 예를 들어, 아래 그림과 같이 입력층의 $$x_{11}$$ 노드에서 은닉층으로 이어지는 연결만 보더라도, 연결 하나 당 한 개의 가중치 값이 붙으므로 총 $$h \times w$$개의 가중치를 고려해야 합니다. 입력층에는 총 $$h \times w$$개의 노드가 존재하므로, 인접한 두 층 사이에만 총 $$h^2w^2$$개의 가중치가 필요합니다. 

{% include image.html name=page.name file="2d-deep-neural-network-connection-example.svg" description="2차원적 완전 연결 신경망에서 노드 하나에서 출발한 연결" class="medium-image" %}

연결 개수의 증가에 따라 가중치의 개수가 기하급수적으로 증가한다는 문제 때문에, 머신러닝 연구자들은 인접한 층 간의 모든 노드의 연결을 고려하는 대신, 크기가 작은 **필터(filter)**가 존재한다고 가정하고 아래 그림과 같이 필터가 겹쳐지는 부분에 대해서만 가중합 및 활성함수 연산을 하도록 하였습니다.

{% include image.html name=page.name file="convolutional-neural-network.svg" description="컨볼루션 신경망에서의 필터에 대한 연산" class="full-image" %}

위 그림에서는 $$3 \times 3$$ 크기의 필터가 입력층의 가장 좌측 상단 $$3 \times 3$$ 영역에 적용되어, 이들 노드에 대한 가중합 및 활성함수 연산을 수행하고 그 출력값을 $$z_{22}$$에 저장하고 있습니다. 이렇게 필터는 원본 입력층 상에서 일정 간격만큼 횡적/종적으로 이동하면서 가중합 및 활성함수 연산을 수행하고, 그 출력값을 현재 필터의 위치에 놓습니다. 이러한 연산 방식은 컴퓨터 비전(computer vision) 분야에서 이미지에 대한 <a href="https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1" target="_blank">컨볼루션(convolution)</a> 연산과 유사하여, 이러한 구조를 채택하는 심층 신경망을 특별히 **컨볼루션 신경망(convolutional neural network)**라고 부르게 되었습니다. 

위 그림에서는 설명의 편의를 위해 하나의 필터만이 작동하는 것을 표현하였으나, 실제로는 인접한 층 사이에 복수 개의 필터를 설치하고 각 필터의 컨볼루션 연산을 통해 복수 개의 은닉층이 생성되도록 합니다. 

{% include image.html name=page.name file="lenet-architecture.svg" description="컨볼루션 신경망의 최초 모델: LeNet" class="full-image" %}

컨볼루션 신경망은, 본래 2차원적 속성을 지니는 데이터에 효과적으로 적용할 수 있습니다. 가로/세로 방향으로 픽셀(pixel)이 분포해 있는 이미지 데이터가 가장 대표적인 사례라고 할 수 있습니다. 실제로도 **이미지 인식(image recognition)** 분야에서 컨볼루션 신경망이 가장 활발하게 사용됩니다. 컨볼루션 신경망에 대한 보다 자세한 내용은, 추후에 또 다른 블로그 포스팅을 통해 다루도록 하겠습니다.

### 순환 신경망

TODO

---


## 딥러닝의 강점 

지금까지는 딥러닝의 개념을 설명하기 위한 내용이었고, 이제 딥러닝이 왜 좋다고 하는지

TODO

### 요인 추출의 자동화

- 요인 추출 자동화: 맨 마지막 은닉층의 결과물이, 곧 추출된 요인

#### 예시1: 나무 여부 구별 기계

- 예를 들어, '사물 윗쪽 중앙 언저리 평균 색상'과 '사물 아랫쪽 중앙 언저리 평균 색상'이, 나무 여부 구별을 위한 '최적의 요인'이었다고 가정 (이는 사실 사람도 모르고, 신(?)만이 아는 사실이나, 일단 알고 있다고 가정)
- 15,000x2x1 구조의 심층 신경망을 생각하면, 이에 대한 학습이 완료되었을 때 은닉층의 2개의 노드(node)에서 이것을 자동으로 추출하는 가중치가 형성됨
- 만약 '최적의 요인'이 색상 요소 외에도 경계, 그림자 등의 요소가 포함되어 있다고 하면, 그 개수만큼 은닉층의 노드를 증가시키면 효과적으로 학습될 것임
- 결과적으로, 모든 예시들을 심층 신경망에 입력하였을 때 얻어지는 맨 마지막 은닉층의 결과물을 공간 상의 점으로 찍어보았을 때, 선형 모델로 예쁘게 분류될 수 있도록 하는 게 최종 목표(선형 모델에서는 불가능했던 경계 표현이 가능해짐)

> 조합을 통한 시너지, 백지장도 맞들면 낫다


### 요인 추출의 자동화가 가져온 변화
- 월등한 인식 성능
- 자동화된 요인 추출 --> 커스터마이징이 가능해짐
- 요인 추출은 엄청난 장인 정신 요구 --> 이 전제가 깨짐


## 딥러닝의 약점

(딥러닝 만능주의를 경계 - 아직 기계 제국이 지배하는 시대는 한참 멀었다)

### 데이터
- 대단히 데이터 의존적

### 속도 및 사양
- 느린 속도
- 고사양 필요


## 결론


## References

- <a href="https://en.wikipedia.org/wiki/Neuron" target="_blank">뉴런의 구조</a>
- 컨볼루션 신경망의 최초 모델: LeNet
  - <a href="http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf" target="_blank">LeCun, Yann, et al. "Gradient-based learning applied to document recognition." Proceedings of the IEEE 86.11 (1998): 2278-2324.</a>
